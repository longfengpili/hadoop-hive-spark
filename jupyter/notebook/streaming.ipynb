{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b488108-804c-4615-95f8-ddc3c0964306",
   "metadata": {},
   "source": [
    "# [structured streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#quick-example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17a0fb2-013d-4968-8ca3-1f8d11fa6f25",
   "metadata": {},
   "source": [
    "在 Apache Spark Structured Streaming 中支持的一些主要数据源格式及其使用示例如下：\r\n",
    "\r\n",
    "1. **文本文件（Text）**：\r\n",
    "   ```python\r\n",
    "   text_df = spark.readStream.format(\"text\").load(\"/path/to/directory\")\r\n",
    "   ```\r\n",
    "\r\n",
    "2. **CSV 文件**：\r\n",
    "   ```python\r\n",
    "   csv_df = spark.readStream.format(\"csv\").option(\"header\", \"true\").load(\"/path/to/directory\")\r\n",
    "   ```\r\n",
    "\r\n",
    "3. **JSON 文件**：\r\n",
    "   ```python\r\n",
    "   json_df = spark.readStream.format(\"json\").load(\"/path/to/directory\")\r\n",
    "   ```\r\n",
    "\r\n",
    "4. **ORC 文件**：\r\n",
    "   ```python\r\n",
    "   orc_df = spark.readStream.format(\"orc\").load(\"/path/to/directory\")\r\n",
    "   ```\r\n",
    "\r\n",
    "5. **Parquet 文件**：\r\n",
    "   ```python\r\n",
    "   parquet_df = spark.readStream.format(\"parquet\").load(\"/path/to/directory\")\r\n",
    "   ```\r\n",
    "\r\n",
    "6. **从 Kafka 读取**：\r\n",
    "   ```python\r\n",
    "   kafka_df = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"host:port\").option(\"subscribe\", \"topic\").load()\r\n",
    "   ```\r\n",
    "\r\n",
    "7. **套接字（Socket，用于测试）**：\r\n",
    "   ```python\r\n",
    "   socket_df = spark.readStream.format(\"socket\").option(\"host\", \"localhost\").option(\"port\", 9999).load()\r\n",
    "   ```\r\n",
    "\r\n",
    "8. **Rate（用于测试）**：\r\n",
    "   ```python\r\n",
    "   rate_df = spark.readStream.format(\"rate\").load()\r\n",
    "   ```\r\n",
    "\r\n",
    "在分布式环境中， **确保指定的路径或资源对所有 Spark 节点可访问**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4457250f-6bab-4ea8-b801-33179644ae11",
   "metadata": {},
   "source": [
    "# csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4532b2f6-2a52-4522-834a-6375930b442d",
   "metadata": {},
   "source": [
    "## csv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00fd5045-39e5-49aa-9357-21fe23b0294b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T02:11:51.500689Z",
     "iopub.status.busy": "2023-11-29T02:11:51.500452Z",
     "iopub.status.idle": "2023-11-29T02:12:01.868416Z",
     "shell.execute_reply": "2023-11-29T02:12:01.867508Z",
     "shell.execute_reply.started": "2023-11-29T02:11:51.500666Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder.appName(\"StructuredStreamingCSV1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31eea3aa-153d-4218-9166-15b144e24737",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T02:12:01.870676Z",
     "iopub.status.busy": "2023-11-29T02:12:01.870183Z",
     "iopub.status.idle": "2023-11-29T02:12:03.274249Z",
     "shell.execute_reply": "2023-11-29T02:12:03.273472Z",
     "shell.execute_reply.started": "2023-11-29T02:12:01.870651Z"
    }
   },
   "outputs": [],
   "source": [
    "file = 'file:///home/jupyter/data/test'\n",
    "\n",
    "# 定义 CSV 文件的 schema\n",
    "schema = StructType([\n",
    "    StructField('time', StringType()),\n",
    "    StructField('app_id', StringType()),\n",
    "    StructField('store', StringType()),\n",
    "    StructField('adid', StringType()),\n",
    "    StructField('openid', StringType()),\n",
    "    StructField('activity_kind', StringType()),\n",
    "    StructField('created_at', StringType()),\n",
    "    StructField('installed_at', StringType()),\n",
    "    StructField('reattributed_at', StringType()),\n",
    "    StructField('network_name', StringType()),\n",
    "    StructField('country', StringType()),\n",
    "    StructField('device_name', StringType()),\n",
    "    StructField('device_type', StringType()),\n",
    "    StructField('os_name', StringType()),\n",
    "    StructField('timezone', StringType()),\n",
    "    StructField('event_name', StringType()),\n",
    "    StructField('revenue_float', StringType()),\n",
    "    StructField('revenue', StringType()),\n",
    "    StructField('currency', StringType()),\n",
    "    StructField('revenue_usd', StringType()),\n",
    "    StructField('reporting_revenue', StringType())\n",
    "])\n",
    "\n",
    "\n",
    "# 读取 CSV 文件\n",
    "csvDF = spark.readStream \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8e62c39-64ba-472b-adb9-72dbdd9df104",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T02:27:02.760467Z",
     "iopub.status.busy": "2023-11-29T02:27:02.759715Z",
     "iopub.status.idle": "2023-11-29T02:27:06.277089Z",
     "shell.execute_reply": "2023-11-29T02:27:06.276283Z",
     "shell.execute_reply.started": "2023-11-29T02:27:02.760440Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/29 02:27:02 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-c1eef534-f62d-467d-beb5-ebef7a2e9062. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/11/29 02:27:02 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------------------+----------+------+--------------------+------+-------------+----------+------------+---------------+------------+-------+-----------+-----------+-------+--------+----------+-------------+-------+--------+-----------+-----------------+\n",
      "|               time|    app_id| store|                adid|openid|activity_kind|created_at|installed_at|reattributed_at|network_name|country|device_name|device_type|os_name|timezone|event_name|revenue_float|revenue|currency|revenue_usd|reporting_revenue|\n",
      "+-------------------+----------+------+--------------------+------+-------------+----------+------------+---------------+------------+-------+-----------+-----------+-------+--------+----------+-------------+-------+--------+-----------+-----------------+\n",
      "|2023-10-01 00:00:00|1456241577|itunes|041bf78c9dc6dd5f5...|  NULL|      session|      NULL|  1636532102|           NULL|     RWD-ady|     jp|       NULL|       NULL|    ios|UTC+0900|      NULL|         NULL|   NULL|    NULL|       NULL|             NULL|\n",
      "+-------------------+----------+------+--------------------+------+-------------+----------+------------+---------------+------------+-------+-----------+-----------+-------+--------+----------+-------------+-------+--------+-----------+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for data to arrive',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'id': '4476cf60-6d08-4e51-9fc2-b1d2d0a72387',\n",
       " 'runId': '0c6a985e-4eed-4305-a23e-d6ee23fe50ff',\n",
       " 'name': None,\n",
       " 'timestamp': '2023-11-29T02:27:02.809Z',\n",
       " 'batchId': 0,\n",
       " 'numInputRows': 1,\n",
       " 'inputRowsPerSecond': 0.0,\n",
       " 'processedRowsPerSecond': 4.291845493562231,\n",
       " 'durationMs': {'addBatch': 113,\n",
       "  'commitOffsets': 31,\n",
       "  'getBatch': 15,\n",
       "  'latestOffset': 40,\n",
       "  'queryPlanning': 6,\n",
       "  'triggerExecution': 233,\n",
       "  'walCommit': 26},\n",
       " 'stateOperators': [],\n",
       " 'sources': [{'description': 'FileStreamSource[file:/home/jupyter/data/test]',\n",
       "   'startOffset': None,\n",
       "   'endOffset': {'logOffset': 0},\n",
       "   'latestOffset': None,\n",
       "   'numInputRows': 1,\n",
       "   'inputRowsPerSecond': 0.0,\n",
       "   'processedRowsPerSecond': 4.291845493562231}],\n",
       " 'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@3e050195',\n",
       "  'numOutputRows': 1}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义数据处理逻辑\n",
    "# 例如，简单的转换或聚合操作\n",
    "\n",
    "# 定义输出接收器，例如输出到控制台\n",
    "query = csvDF.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# 等待流处理结束\n",
    "query.awaitTermination(timeout=3)\n",
    "query.status\n",
    "query.stop()\n",
    "query.lastProgress\n",
    "query.status\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef66a43d-7c84-4618-8b46-3b09f2640625",
   "metadata": {},
   "source": [
    "## csv2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25f6a91f-7892-4dec-9f36-dbac59c0db33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T02:27:23.452400Z",
     "iopub.status.busy": "2023-11-29T02:27:23.452119Z",
     "iopub.status.idle": "2023-11-29T02:27:30.485868Z",
     "shell.execute_reply": "2023-11-29T02:27:30.484893Z",
     "shell.execute_reply.started": "2023-11-29T02:27:23.452376Z"
    }
   },
   "outputs": [],
   "source": [
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder.appName(\"StructuredStreamingCSV2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6bbe28e-85c8-4b57-b516-9685f492e01b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T02:29:05.460675Z",
     "iopub.status.busy": "2023-11-29T02:29:05.460151Z",
     "iopub.status.idle": "2023-11-29T02:29:05.536076Z",
     "shell.execute_reply": "2023-11-29T02:29:05.534919Z",
     "shell.execute_reply.started": "2023-11-29T02:29:05.460649Z"
    }
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Schema must be specified when creating a streaming source DataFrame. If some files already exist in the directory, then depending on the file format you may be able to create a static DataFrame on that directory with 'spark.read.load(directory)' and infer schema from it.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile:///home/jupyter/data/test\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 读取 CSV 文件\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m csvdf \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfalse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/streaming/readwriter.py:302\u001b[0m, in \u001b[0;36mDataStreamReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(path\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m    299\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVALUE_NOT_NON_EMPTY_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    300\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_value\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(path)},\n\u001b[1;32m    301\u001b[0m         )\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload())\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Schema must be specified when creating a streaming source DataFrame. If some files already exist in the directory, then depending on the file format you may be able to create a static DataFrame on that directory with 'spark.read.load(directory)' and infer schema from it."
     ]
    }
   ],
   "source": [
    "file = 'file:///home/jupyter/data/test'\n",
    "# 读取 CSV 文件\n",
    "csvdf = spark.readStream.format(\"csv\").option(\"header\", \"false\").load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3707958f-cb08-4f68-acf9-c275cdf9621c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
