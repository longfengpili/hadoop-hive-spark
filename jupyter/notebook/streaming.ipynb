{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b488108-804c-4615-95f8-ddc3c0964306",
   "metadata": {},
   "source": [
    "# [structured streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#quick-example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17a0fb2-013d-4968-8ca3-1f8d11fa6f25",
   "metadata": {},
   "source": [
    "在 Apache Spark Structured Streaming 中支持的一些主要数据源格式及其使用示例如下：\r\n",
    "\r\n",
    "1. **文本文件（Text）**：\r\n",
    "   ```python\r\n",
    "   text_df = spark.readStream.format(\"text\").load(\"/path/to/directory\")\r\n",
    "   ```\r\n",
    "\r\n",
    "2. **CSV 文件**：\r\n",
    "   ```python\r\n",
    "   csv_df = spark.readStream.format(\"csv\").option(\"header\", \"true\").load(\"/path/to/directory\")\r\n",
    "   ```\r\n",
    "\r\n",
    "3. **JSON 文件**：\r\n",
    "   ```python\r\n",
    "   json_df = spark.readStream.format(\"json\").load(\"/path/to/directory\")\r\n",
    "   ```\r\n",
    "\r\n",
    "4. **ORC 文件**：\r\n",
    "   ```python\r\n",
    "   orc_df = spark.readStream.format(\"orc\").load(\"/path/to/directory\")\r\n",
    "   ```\r\n",
    "\r\n",
    "5. **Parquet 文件**：\r\n",
    "   ```python\r\n",
    "   parquet_df = spark.readStream.format(\"parquet\").load(\"/path/to/directory\")\r\n",
    "   ```\r\n",
    "\r\n",
    "6. **从 Kafka 读取**：\r\n",
    "   ```python\r\n",
    "   kafka_df = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"host:port\").option(\"subscribe\", \"topic\").load()\r\n",
    "   ```\r\n",
    "\r\n",
    "7. **套接字（Socket，用于测试）**：\r\n",
    "   ```python\r\n",
    "   socket_df = spark.readStream.format(\"socket\").option(\"host\", \"localhost\").option(\"port\", 9999).load()\r\n",
    "   ```\r\n",
    "\r\n",
    "8. **Rate（用于测试）**：\r\n",
    "   ```python\r\n",
    "   rate_df = spark.readStream.format(\"rate\").load()\r\n",
    "   ```\r\n",
    "\r\n",
    "在分布式环境中， **确保指定的路径或资源对所有 Spark 节点可访问**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1cb8ab45-c0df-4876-b6b7-12be97617861",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T06:00:38.348847Z",
     "iopub.status.busy": "2023-12-07T06:00:38.348474Z",
     "iopub.status.idle": "2023-12-07T06:00:38.355964Z",
     "shell.execute_reply": "2023-12-07T06:00:38.354637Z",
     "shell.execute_reply.started": "2023-12-07T06:00:38.348822Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义 CSV 文件的 schema\n",
    "schema1 = StructType([\n",
    "    StructField('time', TimestampType()),\n",
    "    StructField('app_id', StringType()),\n",
    "    StructField('store', StringType()),\n",
    "    StructField('adid', StringType()),\n",
    "    StructField('openid', StringType()),\n",
    "    StructField('activity_kind', StringType()),\n",
    "    StructField('created_at', StringType()),\n",
    "    StructField('installed_at', StringType()),\n",
    "    StructField('reattributed_at', StringType()),\n",
    "    StructField('network_name', StringType()),\n",
    "    StructField('country', StringType()),\n",
    "    StructField('device_name', StringType()),\n",
    "    StructField('device_type', StringType()),\n",
    "    StructField('os_name', StringType()),\n",
    "    StructField('timezone', StringType()),\n",
    "    StructField('event_name', StringType()),\n",
    "    StructField('revenue_float', StringType()),\n",
    "    StructField('revenue', StringType()),\n",
    "    StructField('currency', StringType()),\n",
    "    StructField('revenue_usd', StringType()),\n",
    "    StructField('reporting_revenue', StringType())\n",
    "])\n",
    "\n",
    "# 定义 CSV 文件的 schema\n",
    "schema2 = StructType([\n",
    "    StructField('adid', StringType()),\n",
    "    StructField('store', StringType()),\n",
    "    StructField('time', TimestampNTZType()),\n",
    "    StructField('zone_offset', StringType()),\n",
    "    StructField('activity_kind', StringType()),\n",
    "    StructField('event_name', StringType())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4457250f-6bab-4ea8-b801-33179644ae11",
   "metadata": {},
   "source": [
    "# csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4532b2f6-2a52-4522-834a-6375930b442d",
   "metadata": {},
   "source": [
    "## csv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "00fd5045-39e5-49aa-9357-21fe23b0294b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T06:00:56.317110Z",
     "iopub.status.busy": "2023-12-07T06:00:56.316298Z",
     "iopub.status.idle": "2023-12-07T06:00:56.321267Z",
     "shell.execute_reply": "2023-12-07T06:00:56.319913Z",
     "shell.execute_reply.started": "2023-12-07T06:00:56.317082Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e8e62c39-64ba-472b-adb9-72dbdd9df104",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T06:01:14.650745Z",
     "iopub.status.busy": "2023-12-07T06:01:14.650028Z",
     "iopub.status.idle": "2023-12-07T06:01:18.292466Z",
     "shell.execute_reply": "2023-12-07T06:01:18.291115Z",
     "shell.execute_reply.started": "2023-12-07T06:01:14.650720Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/07 06:01:14 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "23/12/07 06:01:14 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-10daf6f6-70a4-4dab-ac7d-ca6bb9a55d2b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/12/07 06:01:14 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------------------+--------------------+------+--------------------+------+-------------+----------+------------+---------------+--------------------+-------+-----------+-----------+-------+--------+----------+-------------+-------+--------+-----------+-----------------+\n",
      "|               time|              app_id| store|                adid|openid|activity_kind|created_at|installed_at|reattributed_at|        network_name|country|device_name|device_type|os_name|timezone|event_name|revenue_float|revenue|currency|revenue_usd|reporting_revenue|\n",
      "+-------------------+--------------------+------+--------------------+------+-------------+----------+------------+---------------+--------------------+-------+-----------+-----------+-------+--------+----------+-------------+-------+--------+-----------+-----------------+\n",
      "|2023-10-01 00:00:00|          1456241577|itunes|041bf78c9dc6dd5f5...|  NULL|      session|      NULL|  1636532102|           NULL|             RWD-ady|     jp|       NULL|       NULL|    ios|UTC+0900|      NULL|         NULL|   NULL|    NULL|       NULL|             NULL|\n",
      "|2023-10-01 00:00:09|          1456241577|itunes|4c8abb5e79e5291c4...|  NULL|      session|      NULL|  1636646821|           NULL|             Organic|     jp|       NULL|       NULL|    ios|UTC+0900|      NULL|         NULL|   NULL|    NULL|       NULL|             NULL|\n",
      "|2023-10-01 00:00:12|          1456241577|itunes|884ca98e510fb4a1a...|  NULL|      session|      NULL|  1620095149|           NULL|          non-moloco|     jp|       NULL|       NULL|    ios|UTC+0900|      NULL|         NULL|   NULL|    NULL|       NULL|             NULL|\n",
      "|2023-10-01 00:00:15|jp.coolfactory.an...|google|041bf78c9dc6dd5f5...|  NULL|      session|      NULL|  1679598383|           NULL|      Google Ads ACI|     jp|       NULL|       NULL|android|UTC+0900|      NULL|         NULL|   NULL|    NULL|       NULL|             NULL|\n",
      "|2023-10-01 00:00:18|          1456241577|itunes|4c8abb5e79e5291c4...|  NULL|      session|      NULL|  1638368462|           NULL|             Organic|     jp|       NULL|       NULL|    ios|UTC+0900|      NULL|         NULL|   NULL|    NULL|       NULL|             NULL|\n",
      "|2023-10-01 00:00:22|          1456241577|itunes|884ca98e510fb4a1a...|  NULL|      session|      NULL|  1646869256|           NULL|    Apple Search Ads|     tw|       NULL|       NULL|    ios|UTC+0800|      NULL|         NULL|   NULL|    NULL|       NULL|             NULL|\n",
      "|2023-10-01 00:00:23|jp.coolfactory.an...|google|884ca98e510fb4a1a...|  NULL|      session|      NULL|  1642321157|           NULL|             Organic|     id|       NULL|       NULL|android|UTC+0700|      NULL|         NULL|   NULL|    NULL|       NULL|             NULL|\n",
      "|2023-10-01 00:00:24|jp.coolfactory.an...|google|884ca98e510fb4a1a...|  NULL|      session|      NULL|  1607615341|           NULL|      Google Ads ACI|     jp|       NULL|       NULL|android|UTC+0900|      NULL|         NULL|   NULL|    NULL|       NULL|             NULL|\n",
      "|2023-10-01 00:00:29|          1456241577|itunes|4c8abb5e79e5291c4...|  NULL|      session|      NULL|  1628169976|           NULL|non-metaps-fukuro...|     jp|       NULL|       NULL|    ios|UTC+0900|      NULL|         NULL|   NULL|    NULL|       NULL|             NULL|\n",
      "+-------------------+--------------------+------+--------------------+------+-------------+----------+------------+---------------+--------------------+-------+-----------+-----------+-------+--------+----------+-------------+-------+--------+-----------+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for data to arrive',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder.appName(\"StructuredStreamingCSV1\").getOrCreate()\n",
    "\n",
    "file = 'file:///home/jupyter/data/test'\n",
    "\n",
    "# 读取 CSV 文件\n",
    "csvDF = spark.readStream \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .schema(schema1) \\\n",
    "    .csv(file)\n",
    "\n",
    "# 定义数据处理逻辑\n",
    "# 例如，简单的转换或聚合操作\n",
    "\n",
    "# 定义输出接收器，例如输出到控制台\n",
    "query = csvDF.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# 等待流处理结束\n",
    "query.awaitTermination(timeout=3)\n",
    "query.status\n",
    "query.stop()\n",
    "# query.lastProgress\n",
    "query.status\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef66a43d-7c84-4618-8b46-3b09f2640625",
   "metadata": {},
   "source": [
    "## csv2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b6bbe28e-85c8-4b57-b516-9685f492e01b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T06:01:33.515896Z",
     "iopub.status.busy": "2023-12-07T06:01:33.515035Z",
     "iopub.status.idle": "2023-12-07T06:01:44.092220Z",
     "shell.execute_reply": "2023-12-07T06:01:44.090688Z",
     "shell.execute_reply.started": "2023-12-07T06:01:33.515859Z"
    }
   },
   "outputs": [],
   "source": [
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder.appName(\"StructuredStreamingCSV2\").getOrCreate()\n",
    "\n",
    "file = 'file:///home/jupyter/data/test'\n",
    "# 读取 CSV 文件\n",
    "csvdf = spark.readStream.format(\"csv\") \\\n",
    "        .option(\"header\", \"false\") \\\n",
    "        .schema(schema1) \\\n",
    "        .load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3707958f-cb08-4f68-acf9-c275cdf9621c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T06:01:44.238941Z",
     "iopub.status.busy": "2023-12-07T06:01:44.238101Z",
     "iopub.status.idle": "2023-12-07T06:01:54.537623Z",
     "shell.execute_reply": "2023-12-07T06:01:54.536631Z",
     "shell.execute_reply.started": "2023-12-07T06:01:44.238901Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/07 06:01:44 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-9347e172-6369-482e-bfd2-169cdff52d68. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/12/07 06:01:44 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------------------+--------------------+------+--------------------+------+-------------+----------+------------+---------------+--------------------+-------+-----------+-----------+-------+--------+----------+-------------+-------+--------+-----------+-----------------+\n",
      "|               time|              app_id| store|                adid|openid|activity_kind|created_at|installed_at|reattributed_at|        network_name|country|device_name|device_type|os_name|timezone|event_name|revenue_float|revenue|currency|revenue_usd|reporting_revenue|\n",
      "+-------------------+--------------------+------+--------------------+------+-------------+----------+------------+---------------+--------------------+-------+-----------+-----------+-------+--------+----------+-------------+-------+--------+-----------+-----------------+\n",
      "|2023-10-01 00:00:00|          1456241577|itunes|041bf78c9dc6dd5f5...|  NULL|      session|      NULL|  1636532102|           NULL|             RWD-ady|     jp|       NULL|       NULL|    ios|UTC+0900|      NULL|         NULL|   NULL|    NULL|       NULL|             NULL|\n",
      "|2023-10-01 00:00:09|          1456241577|itunes|4c8abb5e79e5291c4...|  NULL|      session|      NULL|  1636646821|           NULL|             Organic|     jp|       NULL|       NULL|    ios|UTC+0900|      NULL|         NULL|   NULL|    NULL|       NULL|             NULL|\n",
      "|2023-10-01 00:00:12|          1456241577|itunes|884ca98e510fb4a1a...|  NULL|      session|      NULL|  1620095149|           NULL|          non-moloco|     jp|       NULL|       NULL|    ios|UTC+0900|      NULL|         NULL|   NULL|    NULL|       NULL|             NULL|\n",
      "|2023-10-01 00:00:15|jp.coolfactory.an...|google|041bf78c9dc6dd5f5...|  NULL|      session|      NULL|  1679598383|           NULL|      Google Ads ACI|     jp|       NULL|       NULL|android|UTC+0900|      NULL|         NULL|   NULL|    NULL|       NULL|             NULL|\n",
      "|2023-10-01 00:00:18|          1456241577|itunes|4c8abb5e79e5291c4...|  NULL|      session|      NULL|  1638368462|           NULL|             Organic|     jp|       NULL|       NULL|    ios|UTC+0900|      NULL|         NULL|   NULL|    NULL|       NULL|             NULL|\n",
      "|2023-10-01 00:00:22|          1456241577|itunes|884ca98e510fb4a1a...|  NULL|      session|      NULL|  1646869256|           NULL|    Apple Search Ads|     tw|       NULL|       NULL|    ios|UTC+0800|      NULL|         NULL|   NULL|    NULL|       NULL|             NULL|\n",
      "|2023-10-01 00:00:23|jp.coolfactory.an...|google|884ca98e510fb4a1a...|  NULL|      session|      NULL|  1642321157|           NULL|             Organic|     id|       NULL|       NULL|android|UTC+0700|      NULL|         NULL|   NULL|    NULL|       NULL|             NULL|\n",
      "|2023-10-01 00:00:24|jp.coolfactory.an...|google|884ca98e510fb4a1a...|  NULL|      session|      NULL|  1607615341|           NULL|      Google Ads ACI|     jp|       NULL|       NULL|android|UTC+0900|      NULL|         NULL|   NULL|    NULL|       NULL|             NULL|\n",
      "|2023-10-01 00:00:29|          1456241577|itunes|4c8abb5e79e5291c4...|  NULL|      session|      NULL|  1628169976|           NULL|non-metaps-fukuro...|     jp|       NULL|       NULL|    ios|UTC+0900|      NULL|         NULL|   NULL|    NULL|       NULL|             NULL|\n",
      "+-------------------+--------------------+------+--------------------+------+-------------+----------+------------+---------------+--------------------+-------+-----------+-----------+-------+--------+----------+-------------+-------+--------+-----------+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for data to arrive',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = csvdf.writeStream.format('console').start()\n",
    "query.awaitTermination(timeout=10)\n",
    "\n",
    "query.status\n",
    "query.stop()\n",
    "# query.lastProgress\n",
    "query.status\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ae2418-66d7-4bb8-972e-da1ca583bb42",
   "metadata": {},
   "source": [
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7046b75d-5304-416d-9402-c9c968bb9030",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T06:01:59.608050Z",
     "iopub.status.busy": "2023-12-07T06:01:59.607472Z",
     "iopub.status.idle": "2023-12-07T06:01:59.611843Z",
     "shell.execute_reply": "2023-12-07T06:01:59.610802Z",
     "shell.execute_reply.started": "2023-12-07T06:01:59.608013Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f59fd57-22d7-44ca-be55-d85bd3bef11b",
   "metadata": {},
   "source": [
    "## outhive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4fc510c5-54dd-46b4-a55a-d6df8c410def",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T06:02:00.974340Z",
     "iopub.status.busy": "2023-12-07T06:02:00.973699Z",
     "iopub.status.idle": "2023-12-07T06:02:10.944053Z",
     "shell.execute_reply": "2023-12-07T06:02:10.942983Z",
     "shell.execute_reply.started": "2023-12-07T06:02:00.974299Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"adjust\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "22612983-6d91-4596-8622-9286ed049d6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T06:05:20.465552Z",
     "iopub.status.busy": "2023-12-07T06:05:20.464574Z",
     "iopub.status.idle": "2023-12-07T06:05:20.570514Z",
     "shell.execute_reply": "2023-12-07T06:05:20.569160Z",
     "shell.execute_reply.started": "2023-12-07T06:05:20.465521Z"
    }
   },
   "outputs": [],
   "source": [
    "file = 'file:///home/jupyter/data/adjust'\n",
    "# 读取 CSV 文件\n",
    "csvdf = spark.readStream.format(\"csv\") \\\n",
    "        .option(\"header\", \"false\") \\\n",
    "        .schema(schema2) \\\n",
    "        .load(file)\n",
    "\n",
    "csvdf = csvdf.withColumn(\"time\", F.to_timestamp(csvdf[\"time\"]))\n",
    "_csvdf = csvdf.withWatermark('time', '10 seconds').groupby(\n",
    "    F.window(csvdf.time, \"1440 minutes\", \"720 minutes\"),\n",
    "    csvdf.adid\n",
    ").agg(\n",
    "    F.count(\"time\").alias(\"count\"),\n",
    "    F.min('time').alias('min_time'),\n",
    "    F.max('time').alias('max_time'),\n",
    "    F.last(\"event_name\").alias(\"last_adid\"),\n",
    "    F.max(\"event_name\").alias(\"max_adid\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dabf4db4-c77b-42ef-ba63-1a1a16802964",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T06:05:21.877905Z",
     "iopub.status.busy": "2023-12-07T06:05:21.876988Z",
     "iopub.status.idle": "2023-12-07T06:05:22.016546Z",
     "shell.execute_reply": "2023-12-07T06:05:22.014795Z",
     "shell.execute_reply.started": "2023-12-07T06:05:21.877873Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/07 06:05:21 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ef3ad6dc-78bb-426f-b8df-7fbb6d817d7b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/12/07 06:05:21 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;\nAggregate [window#5060, adid#5023], [window#5060 AS window#5043, adid#5023, count(time#5035-T10000ms) AS count#5051L, min(time#5035-T10000ms) AS min_time#5053, max(time#5035-T10000ms) AS max_time#5055, last(event_name#5028, false) AS last_adid#5057, max(event_name#5028) AS max_adid#5059]\n+- Filter isnotnull(time#5035)\n   +- Expand [[named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(time#5035, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(time#5035, TimestampType, LongType) - 0) % 43200000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(time#5035, TimestampType, LongType) - 0) % 43200000000) + 43200000000) ELSE ((precisetimestampconversion(time#5035, TimestampType, LongType) - 0) % 43200000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(time#5035, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(time#5035, TimestampType, LongType) - 0) % 43200000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(time#5035, TimestampType, LongType) - 0) % 43200000000) + 43200000000) ELSE ((precisetimestampconversion(time#5035, TimestampType, LongType) - 0) % 43200000000) END) - 0) + 86400000000), LongType, TimestampType))), adid#5023, store#5024, time#5035-T10000ms, zone_offset#5026, activity_kind#5027, event_name#5028], [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(time#5035, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(time#5035, TimestampType, LongType) - 0) % 43200000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(time#5035, TimestampType, LongType) - 0) % 43200000000) + 43200000000) ELSE ((precisetimestampconversion(time#5035, TimestampType, LongType) - 0) % 43200000000) END) - 43200000000), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(time#5035, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(time#5035, TimestampType, LongType) - 0) % 43200000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(time#5035, TimestampType, LongType) - 0) % 43200000000) + 43200000000) ELSE ((precisetimestampconversion(time#5035, TimestampType, LongType) - 0) % 43200000000) END) - 43200000000) + 86400000000), LongType, TimestampType))), adid#5023, store#5024, time#5035-T10000ms, zone_offset#5026, activity_kind#5027, event_name#5028]], [window#5060, adid#5023, store#5024, time#5035-T10000ms, zone_offset#5026, activity_kind#5027, event_name#5028]\n      +- EventTimeWatermark time#5035: timestamp, 10 seconds\n         +- Project [adid#5023, store#5024, to_timestamp(time#5025, None, TimestampType, Some(GMT), false) AS time#5035, zone_offset#5026, activity_kind#5027, event_name#5028]\n            +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@40fdb743,csv,List(),Some(StructType(StructField(adid,StringType,true),StructField(store,StringType,true),StructField(time,TimestampNTZType,true),StructField(zone_offset,StringType,true),StructField(activity_kind,StringType,true),StructField(event_name,StringType,true))),List(),None,Map(header -> false, path -> file:///home/jupyter/data/adjust),None), FileSource[file:///home/jupyter/data/adjust], [adid#5023, store#5024, time#5025, zone_offset#5026, activity_kind#5027, event_name#5028]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite_to_hive\u001b[39m(batch_df, batch_id):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# 对于每个微批数据，将其写入 Hive 表\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     batch_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madjust_update\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[43m_csvdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeachBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite_to_hive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m query\u001b[38;5;241m.\u001b[39mawaitTermination(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     11\u001b[0m query\u001b[38;5;241m.\u001b[39mstatus\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/streaming/readwriter.py:1527\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryName(queryName)\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart(path))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;\nAggregate [window#5060, adid#5023], [window#5060 AS window#5043, adid#5023, count(time#5035-T10000ms) AS count#5051L, min(time#5035-T10000ms) AS min_time#5053, max(time#5035-T10000ms) AS max_time#5055, last(event_name#5028, false) AS last_adid#5057, max(event_name#5028) AS max_adid#5059]\n+- Filter isnotnull(time#5035)\n   +- Expand [[named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(time#5035, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(time#5035, TimestampType, LongType) - 0) % 43200000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(time#5035, TimestampType, LongType) - 0) % 43200000000) + 43200000000) ELSE ((precisetimestampconversion(time#5035, TimestampType, LongType) - 0) % 43200000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(time#5035, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(time#5035, TimestampType, LongType) - 0) % 43200000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(time#5035, TimestampType, LongType) - 0) % 43200000000) + 43200000000) ELSE ((precisetimestampconversion(time#5035, TimestampType, LongType) - 0) % 43200000000) END) - 0) + 86400000000), LongType, TimestampType))), adid#5023, store#5024, time#5035-T10000ms, zone_offset#5026, activity_kind#5027, event_name#5028], [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(time#5035, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(time#5035, TimestampType, LongType) - 0) % 43200000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(time#5035, TimestampType, LongType) - 0) % 43200000000) + 43200000000) ELSE ((precisetimestampconversion(time#5035, TimestampType, LongType) - 0) % 43200000000) END) - 43200000000), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(time#5035, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(time#5035, TimestampType, LongType) - 0) % 43200000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(time#5035, TimestampType, LongType) - 0) % 43200000000) + 43200000000) ELSE ((precisetimestampconversion(time#5035, TimestampType, LongType) - 0) % 43200000000) END) - 43200000000) + 86400000000), LongType, TimestampType))), adid#5023, store#5024, time#5035-T10000ms, zone_offset#5026, activity_kind#5027, event_name#5028]], [window#5060, adid#5023, store#5024, time#5035-T10000ms, zone_offset#5026, activity_kind#5027, event_name#5028]\n      +- EventTimeWatermark time#5035: timestamp, 10 seconds\n         +- Project [adid#5023, store#5024, to_timestamp(time#5025, None, TimestampType, Some(GMT), false) AS time#5035, zone_offset#5026, activity_kind#5027, event_name#5028]\n            +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@40fdb743,csv,List(),Some(StructType(StructField(adid,StringType,true),StructField(store,StringType,true),StructField(time,TimestampNTZType,true),StructField(zone_offset,StringType,true),StructField(activity_kind,StringType,true),StructField(event_name,StringType,true))),List(),None,Map(header -> false, path -> file:///home/jupyter/data/adjust),None), FileSource[file:///home/jupyter/data/adjust], [adid#5023, store#5024, time#5025, zone_offset#5026, activity_kind#5027, event_name#5028]\n"
     ]
    }
   ],
   "source": [
    "def write_to_hive(batch_df, batch_id):\n",
    "    # 对于每个微批数据，将其写入 Hive 表\n",
    "    batch_df.write.mode('append').saveAsTable(\"adjust_update\")\n",
    "\n",
    "query = _csvdf.writeStream \\\n",
    "    .foreachBatch(write_to_hive) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination(timeout=10)\n",
    "query.status\n",
    "# query.lastProgress\n",
    "# query.stop()  # 如果没有写完数据，直接stop会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5a5ce3b8-186e-4c5b-8b6d-4167f84889e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T06:02:36.721411Z",
     "iopub.status.busy": "2023-12-07T06:02:36.720310Z",
     "iopub.status.idle": "2023-12-07T06:02:36.731828Z",
     "shell.execute_reply": "2023-12-07T06:02:36.730737Z",
     "shell.execute_reply.started": "2023-12-07T06:02:36.721372Z"
    }
   },
   "outputs": [],
   "source": [
    "query.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1e86f8-e7ad-4a7a-9815-ee6c23af1678",
   "metadata": {},
   "source": [
    "## outmysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "786c7be2-0851-4a57-8aac-59b10e26a424",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T04:08:54.896796Z",
     "iopub.status.busy": "2023-12-05T04:08:54.896122Z",
     "iopub.status.idle": "2023-12-05T04:09:08.521833Z",
     "shell.execute_reply": "2023-12-05T04:09:08.521086Z",
     "shell.execute_reply.started": "2023-12-05T04:08:54.896765Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"adjust_outmysql\") \\\n",
    "    .config(\"spark.jars\", \"/home/jupyter/data/jbdc/mysql-connector-j-8.2.0.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "file = 'file:///home/jupyter/data/adjust'\n",
    "\n",
    "# 定义 CSV 文件的 schema\n",
    "schema = StructType([\n",
    "    StructField('adid', StringType()),\n",
    "    StructField('store', StringType()),\n",
    "    StructField('time', TimestampNTZType()),\n",
    "    StructField('zone_offset', StringType()),\n",
    "    StructField('activity_kind', StringType()),\n",
    "    StructField('event_name', StringType())\n",
    "])\n",
    "\n",
    "# 读取 CSV 文件\n",
    "csvdf = spark.readStream.format(\"csv\") \\\n",
    "        .option(\"header\", \"false\") \\\n",
    "        .schema(schema) \\\n",
    "        .load(file)\n",
    "\n",
    "csvdf = csvdf.withColumn(\"time\", F.to_timestamp(csvdf[\"time\"]))\n",
    "_csvdf = csvdf.withWatermark('time', '10 seconds').groupby(\n",
    "    F.window(csvdf.time, \"1440 minutes\", \"720 minutes\"),\n",
    "    csvdf.adid\n",
    ").agg(\n",
    "    F.count(\"time\").alias(\"count\"),\n",
    "    F.min('time').alias('min_time'),\n",
    "    F.max('time').alias('max_time'),\n",
    "    F.last(\"event_name\").alias(\"last_adid\"),\n",
    "    F.max(\"event_name\").alias(\"max_adid\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8fb18d3-3b65-4531-b651-e89b49d6ef89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T04:09:15.015320Z",
     "iopub.status.busy": "2023-12-05T04:09:15.015054Z",
     "iopub.status.idle": "2023-12-05T04:09:25.229350Z",
     "shell.execute_reply": "2023-12-05T04:09:25.228227Z",
     "shell.execute_reply.started": "2023-12-05T04:09:15.015298Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/05 04:09:15 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-3c3808c3-f0be-4e64-97e6-3be7c2470dbd. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/12/05 04:09:15 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'message': 'Processing new data',\n",
       " 'isDataAvailable': True,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def write_to_mysql(batch_df, batch_id):\n",
    "    # JDBC URL\n",
    "    jdbc_url = \"jdbc:mysql://172.18.0.2:3306/test\"\n",
    "    properties = {\n",
    "        \"user\": \"longfengpili\",\n",
    "        \"password\": \"123456abc\",\n",
    "        \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "    }\n",
    "\n",
    "    # 将每个批次写入 MySQL\n",
    "    batch_df.write.jdbc(url=jdbc_url, table=\"sparkstream\", mode=\"append\", properties=properties)\n",
    "\n",
    "# 设置 Streaming 写入\n",
    "query = _csvdf.writeStream \\\n",
    "    .foreachBatch(write_to_mysql) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination(timeout=10)\n",
    "query.status\n",
    "# query.lastProgress\n",
    "# query.stop()  # 如果没有写完数据，直接stop会报错"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b406c9-c16f-415d-b476-f706f8e20243",
   "metadata": {},
   "source": [
    "## outputMode\n",
    "在 Spark Structured Streaming 中，`outputMode` 定义了如何输出批次数据到下游系统。主要有三种模式：\r\n",
    "\r\n",
    "1. **Append Mode**：仅输出自上次触发以来新增的行。适用于不更改现有数据的场景，如实时日志处理。\r\n",
    "\r\n",
    "2. **Complete Mode**：输出整个结果表。适用于结果表经常更改的场景，如实时聚合统计。\r\n",
    "\r\n",
    "3. **Update Mode**：仅输出自上次触发以来更改的行。适用于只关心更改数\r\n",
    "```\r\n",
    "\r\n",
    "请根据你的实际数据源和业务需求调整代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "533daaa0-a363-4f48-95f9-04e283bdea24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T05:45:00.515211Z",
     "iopub.status.busy": "2023-12-07T05:45:00.514296Z",
     "iopub.status.idle": "2023-12-07T05:45:08.694317Z",
     "shell.execute_reply": "2023-12-07T05:45:08.693003Z",
     "shell.execute_reply.started": "2023-12-07T05:45:00.515180Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"output\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cb233e35-43ea-480f-a4b5-dd14841290ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T05:57:09.577918Z",
     "iopub.status.busy": "2023-12-07T05:57:09.577400Z",
     "iopub.status.idle": "2023-12-07T05:57:09.626813Z",
     "shell.execute_reply": "2023-12-07T05:57:09.624769Z",
     "shell.execute_reply.started": "2023-12-07T05:57:09.577890Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "file = 'file:///home/jupyter/data/test'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fa2fd1-6b51-4f18-a75f-bd6f723201e1",
   "metadata": {},
   "source": [
    "### append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "127551d5-e08a-4918-aab2-1ef4d24a3c84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T05:57:15.347286Z",
     "iopub.status.busy": "2023-12-07T05:57:15.346801Z",
     "iopub.status.idle": "2023-12-07T05:57:45.475316Z",
     "shell.execute_reply": "2023-12-07T05:57:45.473992Z",
     "shell.execute_reply.started": "2023-12-07T05:57:15.347263Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/07 05:57:15 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1aebd474-e56c-4e54-a0a3-e3de7b50934a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/12/07 05:57:15 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------------------+-----+-------------------+-------------------+\n",
      "|                adid|count|           min_time|           max_time|\n",
      "+--------------------+-----+-------------------+-------------------+\n",
      "|884ca98e510fb4a1a...|    4|2023-10-01 00:00:12|2023-10-01 00:00:24|\n",
      "|041bf78c9dc6dd5f5...|    2|2023-10-01 00:00:00|2023-10-01 00:00:15|\n",
      "|4c8abb5e79e5291c4...|    3|2023-10-01 00:00:09|2023-10-01 00:00:29|\n",
      "+--------------------+-----+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取 CSV 文件\n",
    "csvdf = spark.readStream.format(\"csv\") \\\n",
    "        .option(\"header\", \"false\") \\\n",
    "        .schema(schema) \\\n",
    "        .load(file)\n",
    "\n",
    "csvdf = csvdf.withWatermark('time', '10 second')\n",
    "\n",
    "csvdf = csvdf.groupby(\n",
    "    csvdf.adid\n",
    ").agg(\n",
    "    F.count(\"time\").alias(\"count\"),\n",
    "    F.min(\"time\").alias(\"min_time\"),\n",
    "    F.max(\"time\").alias(\"max_time\")\n",
    ")\n",
    "\n",
    "query_append = csvdf.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query_append.awaitTermination(timeout=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6986f773-c881-47fb-90bc-5cc6060b10f1",
   "metadata": {},
   "source": [
    "### append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd1478a-63d2-4037-99f1-d04013418750",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-05T03:40:03.287690Z",
     "iopub.status.idle": "2023-12-05T03:40:03.287943Z",
     "shell.execute_reply": "2023-12-05T03:40:03.287831Z",
     "shell.execute_reply.started": "2023-12-05T03:40:03.287820Z"
    }
   },
   "outputs": [],
   "source": [
    "_csvdf = csvdf.withWatermark(\"time\", \"60 second\")\n",
    "_csvdf = _csvdf.groupby('time').agg(\n",
    "    F.count(\"time\").alias(\"count\"),\n",
    "    F.last(\"adid\").alias(\"last_adid\"),\n",
    "    F.max(\"adid\").alias(\"max_adid\")\n",
    ")\n",
    "# query = _csvdf.writeStream.outputMode('complete').format('console').start()\n",
    "query = _csvdf.writeStream \\\n",
    "        .outputMode('append') \\\n",
    "        .format('csv') \\\n",
    "        .option('path', 'file:///home/jupyter/notebook/output/path') \\\n",
    "        .option('checkpointLocation', 'file:///home/jupyter/notebook/output/checkpointLocation') \\\n",
    "        .start()\n",
    "query.awaitTermination(timeout=10)\n",
    "\n",
    "query.status\n",
    "# query.lastProgress\n",
    "query.stop()\n",
    "query.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1790b3-5d3c-45ec-91db-828a8fa3363f",
   "metadata": {},
   "source": [
    "### update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c9dd59-2bb8-4ab2-82f0-00665214eaa9",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-05T03:40:03.289207Z",
     "iopub.status.idle": "2023-12-05T03:40:03.289473Z",
     "shell.execute_reply": "2023-12-05T03:40:03.289349Z",
     "shell.execute_reply.started": "2023-12-05T03:40:03.289338Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "_csvdf = csvdf.groupby('time').agg(\n",
    "    F.count(\"time\").alias(\"count\"),\n",
    "    F.last(\"adid\").alias(\"last_adid\"),\n",
    "    F.max(\"adid\").alias(\"max_adid\")\n",
    ")\n",
    "query = _csvdf.writeStream.outputMode('update').format('console').start()\n",
    "\n",
    "query.awaitTermination(timeout=10)\n",
    "\n",
    "query.status\n",
    "# query.lastProgress\n",
    "query.stop()\n",
    "query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c869b927-89ce-4628-80b5-083abb476385",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-05T03:40:03.290894Z",
     "iopub.status.idle": "2023-12-05T03:40:03.291173Z",
     "shell.execute_reply": "2023-12-05T03:40:03.291063Z",
     "shell.execute_reply.started": "2023-12-05T03:40:03.291052Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaebb5b-cbd5-4cb3-8931-9290182c649d",
   "metadata": {},
   "source": [
    "# window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1886c734-aa36-4b01-b720-f19c06e5a1df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
