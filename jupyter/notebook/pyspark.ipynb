{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a2104ea-2764-463e-ac2d-929322a6a143",
   "metadata": {},
   "source": [
    "# local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a5fab7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T06:43:01.473889Z",
     "iopub.status.busy": "2023-11-22T06:43:01.473680Z",
     "iopub.status.idle": "2023-11-22T06:43:01.480962Z",
     "shell.execute_reply": "2023-11-22T06:43:01.479558Z",
     "shell.execute_reply.started": "2023-11-22T06:43:01.473869Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52921414",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T06:43:01.482901Z",
     "iopub.status.busy": "2023-11-22T06:43:01.482256Z",
     "iopub.status.idle": "2023-11-22T06:43:01.664258Z",
     "shell.execute_reply": "2023-11-22T06:43:01.663495Z",
     "shell.execute_reply.started": "2023-11-22T06:43:01.482873Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef9d4d47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T06:43:01.665686Z",
     "iopub.status.busy": "2023-11-22T06:43:01.665470Z",
     "iopub.status.idle": "2023-11-22T06:43:17.301896Z",
     "shell.execute_reply": "2023-11-22T06:43:17.301108Z",
     "shell.execute_reply.started": "2023-11-22T06:43:01.665668Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55fad11f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T06:43:17.303561Z",
     "iopub.status.busy": "2023-11-22T06:43:17.302960Z",
     "iopub.status.idle": "2023-11-22T06:43:21.443824Z",
     "shell.execute_reply": "2023-11-22T06:43:21.443068Z",
     "shell.execute_reply.started": "2023-11-22T06:43:17.303527Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/22 06:43:19 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/11/22 06:43:19 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/11/22 06:43:20 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/11/22 06:43:20 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "23/11/22 06:43:20 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/11/22 06:43:20 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/11/22 06:43:20 WARN HiveMetaStore: Location: hdfs://master/user/hive/warehouse/src specified for non-external table:src\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/22 06:43:21 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create table if not exists src (key int, value string) using hive\")\n",
    "spark.sql(\"load data local inpath '/opt/spark/examples/src/main/resources/kv1.txt' into table src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2db995bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T06:43:21.445055Z",
     "iopub.status.busy": "2023-11-22T06:43:21.444688Z",
     "iopub.status.idle": "2023-11-22T06:43:24.817967Z",
     "shell.execute_reply": "2023-11-22T06:43:24.817188Z",
     "shell.execute_reply.started": "2023-11-22T06:43:21.445036Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|key|  value|\n",
      "+---+-------+\n",
      "|238|val_238|\n",
      "| 86| val_86|\n",
      "|311|val_311|\n",
      "| 27| val_27|\n",
      "|165|val_165|\n",
      "|409|val_409|\n",
      "|255|val_255|\n",
      "|278|val_278|\n",
      "| 98| val_98|\n",
      "|484|val_484|\n",
      "|265|val_265|\n",
      "|193|val_193|\n",
      "|401|val_401|\n",
      "|150|val_150|\n",
      "|273|val_273|\n",
      "|224|val_224|\n",
      "|369|val_369|\n",
      "| 66| val_66|\n",
      "|128|val_128|\n",
      "|213|val_213|\n",
      "+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from src\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01d88f94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T06:43:24.820171Z",
     "iopub.status.busy": "2023-11-22T06:43:24.819954Z",
     "iopub.status.idle": "2023-11-22T06:43:26.884154Z",
     "shell.execute_reply": "2023-11-22T06:43:26.883350Z",
     "shell.execute_reply.started": "2023-11-22T06:43:24.820153Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     500|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from src\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87c682d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T06:43:26.885398Z",
     "iopub.status.busy": "2023-11-22T06:43:26.885170Z",
     "iopub.status.idle": "2023-11-22T06:43:28.649250Z",
     "shell.execute_reply": "2023-11-22T06:43:28.648241Z",
     "shell.execute_reply.started": "2023-11-22T06:43:26.885379Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: 0, value: val_0\n",
      "key: 0, value: val_0\n",
      "key: 0, value: val_0\n",
      "key: 2, value: val_2\n",
      "key: 4, value: val_4\n",
      "key: 5, value: val_5\n",
      "key: 5, value: val_5\n",
      "key: 5, value: val_5\n",
      "key: 8, value: val_8\n",
      "key: 9, value: val_9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sqldf = spark.sql(\"select key, value from src where key < 10 order by key\")\n",
    "stringsds = sqldf.rdd.map(lambda row: f\"key: {row.key}, value: {row.value}\")\n",
    "for record in stringsds.collect():\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9987c966",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T06:43:28.650364Z",
     "iopub.status.busy": "2023-11-22T06:43:28.650156Z",
     "iopub.status.idle": "2023-11-22T06:43:28.724003Z",
     "shell.execute_reply": "2023-11-22T06:43:28.723156Z",
     "shell.execute_reply.started": "2023-11-22T06:43:28.650346Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "Record = Row(\"key\", \"value\")\n",
    "recordsDF = spark.createDataFrame([Record(i, \"val_\" + str(i)) for i in range(1, 101)])\n",
    "recordsDF.createOrReplaceTempView(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3981c7d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T06:43:28.725528Z",
     "iopub.status.busy": "2023-11-22T06:43:28.725231Z",
     "iopub.status.idle": "2023-11-22T06:43:29.703114Z",
     "shell.execute_reply": "2023-11-22T06:43:29.702264Z",
     "shell.execute_reply.started": "2023-11-22T06:43:28.725499Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+\n",
      "|key| value|key| value|\n",
      "+---+------+---+------+\n",
      "|  2| val_2|  2| val_2|\n",
      "|  4| val_4|  4| val_4|\n",
      "|  5| val_5|  5| val_5|\n",
      "|  5| val_5|  5| val_5|\n",
      "|  5| val_5|  5| val_5|\n",
      "|  8| val_8|  8| val_8|\n",
      "|  9| val_9|  9| val_9|\n",
      "| 10|val_10| 10|val_10|\n",
      "| 11|val_11| 11|val_11|\n",
      "| 12|val_12| 12|val_12|\n",
      "| 12|val_12| 12|val_12|\n",
      "| 15|val_15| 15|val_15|\n",
      "| 15|val_15| 15|val_15|\n",
      "| 17|val_17| 17|val_17|\n",
      "| 18|val_18| 18|val_18|\n",
      "| 18|val_18| 18|val_18|\n",
      "| 19|val_19| 19|val_19|\n",
      "| 20|val_20| 20|val_20|\n",
      "| 24|val_24| 24|val_24|\n",
      "| 24|val_24| 24|val_24|\n",
      "+---+------+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from records r join src s on r.key = s.key\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c35ff53b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T06:43:29.704878Z",
     "iopub.status.busy": "2023-11-22T06:43:29.704209Z",
     "iopub.status.idle": "2023-11-22T06:43:30.690103Z",
     "shell.execute_reply": "2023-11-22T06:43:30.689393Z",
     "shell.execute_reply.started": "2023-11-22T06:43:29.704856Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963093d8-3fb8-44b8-b404-7b9a64501dd0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca46916-ea0d-4d62-b421-f187be006afb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
