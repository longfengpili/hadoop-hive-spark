{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a2104ea-2764-463e-ac2d-929322a6a143",
   "metadata": {},
   "source": [
    "# local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a5fab7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T07:43:02.904168Z",
     "iopub.status.busy": "2023-11-23T07:43:02.903903Z",
     "iopub.status.idle": "2023-11-23T07:43:02.914149Z",
     "shell.execute_reply": "2023-11-23T07:43:02.912646Z",
     "shell.execute_reply.started": "2023-11-23T07:43:02.904143Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52921414",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T07:43:02.917171Z",
     "iopub.status.busy": "2023-11-23T07:43:02.916292Z",
     "iopub.status.idle": "2023-11-23T07:43:02.987010Z",
     "shell.execute_reply": "2023-11-23T07:43:02.986270Z",
     "shell.execute_reply.started": "2023-11-23T07:43:02.917135Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef9d4d47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T07:43:02.988275Z",
     "iopub.status.busy": "2023-11-23T07:43:02.988083Z",
     "iopub.status.idle": "2023-11-23T07:43:14.396988Z",
     "shell.execute_reply": "2023-11-23T07:43:14.396202Z",
     "shell.execute_reply.started": "2023-11-23T07:43:02.988256Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('test') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55fad11f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T07:43:14.398690Z",
     "iopub.status.busy": "2023-11-23T07:43:14.398019Z",
     "iopub.status.idle": "2023-11-23T07:43:18.610186Z",
     "shell.execute_reply": "2023-11-23T07:43:18.609261Z",
     "shell.execute_reply.started": "2023-11-23T07:43:14.398665Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/23 07:43:16 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/11/23 07:43:16 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/23 07:43:18 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create table if not exists src (key int, value string) using hive\")\n",
    "spark.sql(\"load data local inpath '/opt/spark/examples/src/main/resources/kv1.txt' into table src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2db995bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T07:43:18.611844Z",
     "iopub.status.busy": "2023-11-23T07:43:18.611185Z",
     "iopub.status.idle": "2023-11-23T07:43:22.137689Z",
     "shell.execute_reply": "2023-11-23T07:43:22.136938Z",
     "shell.execute_reply.started": "2023-11-23T07:43:18.611820Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/23 07:43:19 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|key|  value|\n",
      "+---+-------+\n",
      "|238|val_238|\n",
      "| 86| val_86|\n",
      "|311|val_311|\n",
      "| 27| val_27|\n",
      "|165|val_165|\n",
      "|409|val_409|\n",
      "|255|val_255|\n",
      "|278|val_278|\n",
      "| 98| val_98|\n",
      "|484|val_484|\n",
      "|265|val_265|\n",
      "|193|val_193|\n",
      "|401|val_401|\n",
      "|150|val_150|\n",
      "|273|val_273|\n",
      "|224|val_224|\n",
      "|369|val_369|\n",
      "| 66| val_66|\n",
      "|128|val_128|\n",
      "|213|val_213|\n",
      "+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from src\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01d88f94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T07:43:22.138945Z",
     "iopub.status.busy": "2023-11-23T07:43:22.138665Z",
     "iopub.status.idle": "2023-11-23T07:43:24.269611Z",
     "shell.execute_reply": "2023-11-23T07:43:24.268632Z",
     "shell.execute_reply.started": "2023-11-23T07:43:22.138926Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/23 07:43:22 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "23/11/23 07:43:22 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/11/23 07:43:22 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "[Stage 1:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    1500|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from src\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87c682d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T07:43:24.272681Z",
     "iopub.status.busy": "2023-11-23T07:43:24.272075Z",
     "iopub.status.idle": "2023-11-23T07:43:25.989226Z",
     "shell.execute_reply": "2023-11-23T07:43:25.988456Z",
     "shell.execute_reply.started": "2023-11-23T07:43:24.272657Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: 0, value: val_0\n",
      "key: 0, value: val_0\n",
      "key: 0, value: val_0\n",
      "key: 0, value: val_0\n",
      "key: 0, value: val_0\n",
      "key: 0, value: val_0\n",
      "key: 0, value: val_0\n",
      "key: 0, value: val_0\n",
      "key: 0, value: val_0\n",
      "key: 2, value: val_2\n",
      "key: 2, value: val_2\n",
      "key: 2, value: val_2\n",
      "key: 4, value: val_4\n",
      "key: 4, value: val_4\n",
      "key: 4, value: val_4\n",
      "key: 5, value: val_5\n",
      "key: 5, value: val_5\n",
      "key: 5, value: val_5\n",
      "key: 5, value: val_5\n",
      "key: 5, value: val_5\n",
      "key: 5, value: val_5\n",
      "key: 5, value: val_5\n",
      "key: 5, value: val_5\n",
      "key: 5, value: val_5\n",
      "key: 8, value: val_8\n",
      "key: 8, value: val_8\n",
      "key: 8, value: val_8\n",
      "key: 9, value: val_9\n",
      "key: 9, value: val_9\n",
      "key: 9, value: val_9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sqldf = spark.sql(\"select key, value from src where key < 10 order by key\")\n",
    "stringsds = sqldf.rdd.map(lambda row: f\"key: {row.key}, value: {row.value}\")\n",
    "for record in stringsds.collect():\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9987c966",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T07:43:25.990381Z",
     "iopub.status.busy": "2023-11-23T07:43:25.990181Z",
     "iopub.status.idle": "2023-11-23T07:43:26.057437Z",
     "shell.execute_reply": "2023-11-23T07:43:26.056711Z",
     "shell.execute_reply.started": "2023-11-23T07:43:25.990361Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "Record = Row(\"key\", \"value\")\n",
    "recordsDF = spark.createDataFrame([Record(i, \"val_\" + str(i)) for i in range(1, 101)])\n",
    "recordsDF.createOrReplaceTempView(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3981c7d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T07:43:26.058844Z",
     "iopub.status.busy": "2023-11-23T07:43:26.058421Z",
     "iopub.status.idle": "2023-11-23T07:43:27.115537Z",
     "shell.execute_reply": "2023-11-23T07:43:27.114559Z",
     "shell.execute_reply.started": "2023-11-23T07:43:26.058820Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+-----+\n",
      "|key|value|key|value|\n",
      "+---+-----+---+-----+\n",
      "|  2|val_2|  2|val_2|\n",
      "|  2|val_2|  2|val_2|\n",
      "|  2|val_2|  2|val_2|\n",
      "|  4|val_4|  4|val_4|\n",
      "|  4|val_4|  4|val_4|\n",
      "|  4|val_4|  4|val_4|\n",
      "|  5|val_5|  5|val_5|\n",
      "|  5|val_5|  5|val_5|\n",
      "|  5|val_5|  5|val_5|\n",
      "|  5|val_5|  5|val_5|\n",
      "|  5|val_5|  5|val_5|\n",
      "|  5|val_5|  5|val_5|\n",
      "|  5|val_5|  5|val_5|\n",
      "|  5|val_5|  5|val_5|\n",
      "|  5|val_5|  5|val_5|\n",
      "|  8|val_8|  8|val_8|\n",
      "|  8|val_8|  8|val_8|\n",
      "|  8|val_8|  8|val_8|\n",
      "|  9|val_9|  9|val_9|\n",
      "|  9|val_9|  9|val_9|\n",
      "+---+-----+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from records r join src s on r.key = s.key\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c35ff53b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T07:43:27.116858Z",
     "iopub.status.busy": "2023-11-23T07:43:27.116619Z",
     "iopub.status.idle": "2023-11-23T07:43:28.102378Z",
     "shell.execute_reply": "2023-11-23T07:43:28.101617Z",
     "shell.execute_reply.started": "2023-11-23T07:43:27.116835Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca46916-ea0d-4d62-b421-f187be006afb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
