services:
  metastore:
    image: postgres:13.12
    hostname: metastore
    environment:
      POSTGRES_PASSWORD: jupyter
    ports:
      - "5432:5432"
    volumes:
      - metastore:/var/lib/postgresql/data
      - ddl:/docker-entrypoint-initdb.d
    restart: always
    networks:
      sparknet:
        ipv4_address: 172.28.1.1
    extra_hosts:
      - "master:172.28.1.2"
      - "worker1:172.28.1.3"
      - "worker2:172.28.1.4"
      - "history:172.28.1.5"

  master:
    image: hadoop-hive-spark-master
    hostname: master
    depends_on:
      - metastore
    environment:
      SPARK_MASTER_HOST: 172.28.1.2
      SPARK_LOCAL_IP: 172.28.1.2
      SPARK_LOCAL_HOSTNAME: master
    ports:
      - "8020:8020"
      - "8080:8080"
      - "8088:8088"
      - "9870:9870"
      - "7077:7077"
      - "10000:10000"
    volumes:
      - namenode:/opt/hadoop/dfs/name
      # - mydata:/home/jupyter/data
    restart: always
    networks:
      sparknet:
        ipv4_address: 172.28.1.2
    extra_hosts:
      - "metastore:172.28.1.1"
      - "worker1:172.28.1.3"
      - "worker2:172.28.1.4"
      - "history:172.28.1.5"

  worker1:
    image: hadoop-hive-spark-worker
    hostname: worker1
    depends_on:
      - master
    environment:
      SPARK_MASTER_HOST: 172.28.1.2
      SPARK_LOCAL_IP: 172.28.1.3
      SPARK_LOCAL_HOSTNAME: worker1
    ports:
      - "18042:8042"
      - "18081:8081"
      - "19864:9864"
    volumes:
      - datanode1:/opt/hadoop/dfs/data
      - mydata:/home/jupyter/data
      - outputdata:/home/jupyter/outputdata
    restart: always
    networks:
      sparknet:
        ipv4_address: 172.28.1.3
    extra_hosts:
      - "metastore:172.28.1.1"
      - "master:172.28.1.2"
      - "worker2:172.28.1.4"
      - "history:172.28.1.5"

  worker2:
    image: hadoop-hive-spark-worker
    hostname: worker2
    depends_on:
      - master
    environment:
      SPARK_MASTER_HOST: 172.28.1.2
      SPARK_LOCAL_IP: 172.28.1.4
      SPARK_LOCAL_HOSTNAME: worker2
    ports:
      - "28042:8042"
      - "28081:8081"
      - "29864:9864"
    volumes:
      - datanode2:/opt/hadoop/dfs/data
      - mydata:/home/jupyter/data
      - outputdata:/home/jupyter/outputdata
    restart: always
    networks:
      sparknet:
        ipv4_address: 172.28.1.4
    extra_hosts:
      - "metastore:172.28.1.1"
      - "master:172.28.1.2"
      - "worker1:172.28.1.3"
      - "history:172.28.1.5"

  history:
    image: hadoop-hive-spark-history
    hostname: history
    depends_on:
      - master
      - worker1
      - worker2
    environment:
      SPARK_MASTER_HOST: 172.28.1.2
      SPARK_LOCAL_IP: 172.28.1.5
      SPARK_LOCAL_HOSTNAME: history
    ports:
      - "18080:18080"
      - "19888:19888"
    # volumes:
    #   - mydata:/home/jupyter/data
    restart: always
    networks:
      sparknet:
        ipv4_address: 172.28.1.5
    extra_hosts:
      - "metastore:172.28.1.1"
      - "master:172.28.1.2"
      - "worker1:172.28.1.3"
      - "worker2:172.28.1.4"

  jupyter:
    image: hadoop-hive-spark-jupyter
    hostname: jupyter
    environment:
      SPARK_MASTER_HOST: 172.28.1.2
      SPARK_LOCAL_IP: 172.28.1.6
      SPARK_LOCAL_HOSTNAME: jupyter
    depends_on:
      - master
      - worker1
      - worker2
    ports:
      - "18888:8888"
      - "4040:4040"
    volumes:
      - nbconfig:/home/jupyter/.jupyter
      - iconfig:/home/jupyter/.ipython
      - notebook:/home/jupyter/notebook
      - mydata:/home/jupyter/data
      - outputdata:/home/jupyter/outputdata
    restart: always
    networks:
      sparknet:
        ipv4_address: 172.28.1.6
    extra_hosts:
      - "metastore:172.28.1.1"
      - "master:172.28.1.2"
      - "worker1:172.28.1.3"
      - "worker2:172.28.1.4"
      - "history:172.28.1.5"

volumes:
  metastore:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/postgresql/data
  ddl:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./ddl
  namenode:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/hadoop/dfs/name
  namesecondary:
  datanode1:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/hadoop/dfs/data1
  datanode2:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/hadoop/dfs/data2
  notebook:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./jupyter/notebook
  nbconfig:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./jupyter/nbconfig
  iconfig:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./jupyter/iconfig
  mydata:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./jupyter/data
  outputdata:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/outputdata

networks:
  sparknet:
    ipam:
      driver: default
      config:
        - subnet: 172.28.0.0/16
